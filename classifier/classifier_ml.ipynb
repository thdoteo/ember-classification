{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4_Ccr0Jn5jP"
   },
   "source": [
    "## Link Google Drive to retrieve dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "R1yQCk6ZecxO",
    "outputId": "ac7078b8-59de-49e5-977a-acb905cfc735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVzY8vVPoQ9G"
   },
   "source": [
    "## Import libraries and initialization\n",
    "Set path to the directory that contains the ember dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdHGdm8NeNih"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# path to directory containing the ember directory\n",
    "path = \"/content/drive/My Drive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJjdYPgmYMVk"
   },
   "source": [
    "## Remove unlabeled vectors and create new file\n",
    "**Only run this cell if label-stripped dataset does not exist already.**\n",
    "\n",
    "Change path to the corresponding files if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SrCGt8VDXsSg"
   },
   "outputs": [],
   "source": [
    "# Load file in array.\n",
    "features = np.memmap(path + \"ember-dataset/Xtrain.dat\", dtype=np.float32, mode='r', shape=(900000, 2351))\n",
    "\n",
    "# Load label file.\n",
    "labels = np.memmap(path + \"ember-dataset/Ytrain.dat\", dtype=np.float32, mode='r')\n",
    "\n",
    "# Remove unlabeled vectors\n",
    "unlabeled_index = np.argwhere(labels==-1).flatten()\n",
    "labels = np.delete(labels, unlabeled_index, 0)\n",
    "features = np.delete(features, unlabeled_index, 0)\n",
    "\n",
    "# Generate files from dataset where unlabeled data are removed\n",
    "# Files are saved to the format npy, use np.load() to load them\n",
    "np.save(path + \"Xtrain_no_unlabeled.npy\", features)\n",
    "np.save(path + \"Ytrain_no_unlabeled.npy\", labels)\n",
    "\n",
    "del features\n",
    "del labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8vq9Twygb0LK"
   },
   "source": [
    "## Load dataset stripped from unlabeled vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJ0s8cqZdBPn"
   },
   "outputs": [],
   "source": [
    "# After stripping only 600000 vectors remain\n",
    "features = np.load(path + \"Xtrain_no_unlabeled.npy\", mmap_mode='r+')\n",
    "features = np.reshape(features, (-1, 2351))\n",
    "labels = np.load(path + \"Ytrain_no_unlabeled.npy\", mmap_mode='r+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-NR1RUgezUfZ"
   },
   "source": [
    "## Compression using PCA (Principal Component Analysis)\n",
    "- Test with PCA\n",
    "\n",
    "- Use of IncrementalPCA\n",
    "\n",
    "    - allow to compress using minibatches the dataset.\n",
    "\n",
    "    - partial_fit seems to use less RAM than PCA's fit. However, when getting to transform operation it consumes a lot of RAM.\n",
    "\n",
    "- Going back to PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aejj4-XVzRsc"
   },
   "outputs": [],
   "source": [
    "size = len(features)\n",
    "# Define PCA and with the dimension to which it needs be reduced to\n",
    "pca = PCA(n_components=500)\n",
    "# Allows to center the points\n",
    "# Tried StandardScaler and MinMaxScaler\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "features = pca.fit_transform(features_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7cehpQto7ei4"
   },
   "source": [
    "## Split dataset for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "lDaYS1EF7IS3",
    "outputId": "ec5c2737-29e0-4860-d410-1e7b4c10d9c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (420000, 500)\n",
      "Train_labels shape: (420000,)\n",
      "Test shape: (180000, 500)\n",
      "Test_labels shape: (180000,)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into two subsets, one for training, another for testing\n",
    "# test subset contains a third of the original dataset, train contains the rest\n",
    "# Dataset is not shuffled before splitting\n",
    "train, test, train_labels, test_labels = train_test_split(features,\n",
    "                                                          labels,\n",
    "                                                          test_size=0.3,\n",
    "                                                          random_state=1,\n",
    "                                                          shuffle=False)\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Train_labels shape:\", train_labels.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "print(\"Test_labels shape:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6md2Cmw_pPt3"
   },
   "source": [
    "## Build classifier and evaluate performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aROjWyuopPEC",
    "outputId": "90255ed6-59fb-4a35-b23d-3d296abdd1cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.7736277777777778\n"
     ]
    }
   ],
   "source": [
    "# Initialize our classifier\n",
    "# Use Bernoulli distribution as only 2 outputs remain after stripping unlabeled data\n",
    "# either malignant or benign\n",
    "gnb = BernoulliNB()\n",
    "# Train our classifier\n",
    "for i in range(0, len(train), len(train) // 4):\n",
    "    train_subset = train[i : i + len(train) // 4]\n",
    "    train_labels_subset = train_labels[i : i + len(train) // 4]\n",
    "    gnb.partial_fit(train_subset, train_labels_subset, np.unique(train_labels_subset))\n",
    "\n",
    "# Make predictions\n",
    "preds = gnb.predict(test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"Accuracy :\", accuracy_score(test_labels, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O4wwZ2-c62Zj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "classifier-ml.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
