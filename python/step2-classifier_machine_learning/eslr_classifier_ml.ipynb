{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eslr-classifier-ml.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVbfyvtGbztb",
        "colab_type": "text"
      },
      "source": [
        "## Classifier with machine learning (ESLR Recruitment Project 2019)\n",
        "\n",
        "## Resources\n",
        "- https://towardsdatascience.com/an-approach-to-choosing-the-number-of-components-in-a-principal-component-analysis-pca-3b9f3d6e73fe\n",
        "- https://sdsawtelle.github.io/blog/output/week8-andrew-ng-machine-learning-with-python.html#PCA-in-sklearn\n",
        "- https://bogotobogo.com/python/scikit-learn/scikit_machine_learning_Data_Compresssion_via_Dimensionality_Reduction_1_Principal_component_analysis%20_PCA.php\n",
        "\n",
        "## Classification definition\n",
        "\n",
        "Classification is a supervised learning approach. It learns from a given dataset and apply what it has learnt to classify other inputs.\n",
        "\n",
        "## Naives Bayes classifier\n",
        "\n",
        "The Naives Bayes classifier is based on the Bayes Theorem with an assumption of independence among predictors. It is easy to build and is useful in the case of large dataset.\n",
        "Here is used the Bernoulli variant, which assumes that all classes are binary, either malignant or benign in our case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4_Ccr0Jn5jP",
        "colab_type": "text"
      },
      "source": [
        "## Link Google Drive to retrieve dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1yQCk6ZecxO",
        "colab_type": "code",
        "outputId": "58313455-bf4e-4177-8c65-fd51342a50e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_QBH7M7DHl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVzY8vVPoQ9G",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries and initialization\n",
        "Set path to the directory that contains the ember dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdHGdm8NeNih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_files\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import IncrementalPCA, PCA\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# path to directory containing the ember directory\n",
        "path = \"/content/drive/My Drive/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJjdYPgmYMVk",
        "colab_type": "text"
      },
      "source": [
        "## Remove unlabeled vectors and create new file\n",
        "**Only run this cell if label-stripped dataset does not exist already.**\n",
        "\n",
        "Change path to the corresponding files if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrCGt8VDXsSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load file in array.\n",
        "features = np.memmap(path + \"ember-dataset/Xtrain.dat\", dtype=np.float32, mode='r', shape=(900000, 2351))\n",
        "\n",
        "# Load label file.\n",
        "labels = np.memmap(path + \"ember-dataset/Ytrain.dat\", dtype=np.float32, mode='r')\n",
        "\n",
        "# Remove unlabeled vectors\n",
        "unlabeled_index = np.argwhere(labels==-1).flatten()\n",
        "labels = np.delete(labels, unlabeled_index, 0)\n",
        "features = np.delete(features, unlabeled_index, 0)\n",
        "\n",
        "# Generate files from dataset where unlabeled data are removed\n",
        "# Files are saved to the format npy, use np.load() to load them\n",
        "np.save(path + \"Xtrain_no_unlabeled.npy\", features)\n",
        "np.save(path + \"Ytrain_no_unlabeled.npy\", labels)\n",
        "\n",
        "del features\n",
        "del labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vq9Twygb0LK",
        "colab_type": "text"
      },
      "source": [
        "## Load dataset stripped from unlabeled vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ0s8cqZdBPn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After stripping only 600000 vectors remain\n",
        "features = np.load(path + \"Xtrain_no_unlabeled.npy\", mmap_mode='r+')\n",
        "features = np.reshape(features, (-1, 2351))\n",
        "labels = np.load(path + \"Ytrain_no_unlabeled.npy\", mmap_mode='r+')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NR1RUgezUfZ",
        "colab_type": "text"
      },
      "source": [
        "## Compression using PCA (Principal Component Analysis)\n",
        "- Test with PCA\n",
        "\n",
        "- Use of IncrementalPCA\n",
        "\n",
        "    - allow to compress using minibatches the dataset.\n",
        "\n",
        "    - partial_fit seems to use less RAM than PCA's fit. However, when getting to transform operation it consumes a lot of RAM.\n",
        "\n",
        "- Going back to PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aejj4-XVzRsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size = len(features)\n",
        "# Define PCA and with the dimension to which it needs be reduced to\n",
        "pca = PCA(n_components=500)\n",
        "# Allows to center the points\n",
        "# Tried StandardScaler and MinMaxScaler\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "features = pca.fit_transform(features_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cehpQto7ei4",
        "colab_type": "text"
      },
      "source": [
        "## Split dataset for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDaYS1EF7IS3",
        "colab_type": "code",
        "outputId": "ec5c2737-29e0-4860-d410-1e7b4c10d9c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Split dataset into two subsets, one for training, another for testing\n",
        "# test subset contains a third of the original dataset, train contains the rest\n",
        "# Dataset is not shuffled before splitting\n",
        "train, test, train_labels, test_labels = train_test_split(features,\n",
        "                                                          labels,\n",
        "                                                          test_size=0.3,\n",
        "                                                          random_state=1,\n",
        "                                                          shuffle=False)\n",
        "\n",
        "print(\"Train shape:\", train.shape)\n",
        "print(\"Train_labels shape:\", train_labels.shape)\n",
        "print(\"Test shape:\", test.shape)\n",
        "print(\"Test_labels shape:\", test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (420000, 500)\n",
            "Train_labels shape: (420000,)\n",
            "Test shape: (180000, 500)\n",
            "Test_labels shape: (180000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6md2Cmw_pPt3",
        "colab_type": "text"
      },
      "source": [
        "## Build classifier and evaluate performance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aROjWyuopPEC",
        "colab_type": "code",
        "outputId": "90255ed6-59fb-4a35-b23d-3d296abdd1cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Initialize our classifier\n",
        "# Use Bernoulli distribution as only 2 outputs remain after stripping unlabeled data\n",
        "# either malignant or benign\n",
        "gnb = BernoulliNB()\n",
        "# Train our classifier\n",
        "for i in range(0, len(train), len(train) // 4):\n",
        "    train_subset = train[i : i + len(train) // 4]\n",
        "    train_labels_subset = train_labels[i : i + len(train) // 4]\n",
        "    gnb.partial_fit(train_subset, train_labels_subset, np.unique(train_labels_subset))\n",
        "\n",
        "# Make predictions\n",
        "preds = gnb.predict(test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "print(\"Accuracy :\", accuracy_score(test_labels, preds))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.7736277777777778\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
