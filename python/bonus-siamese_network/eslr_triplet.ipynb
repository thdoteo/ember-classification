{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eslr-triplet.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGybJt77xWvE",
        "colab_type": "text"
      },
      "source": [
        "# Triplet network (ESLR Recruitment Project 2019)\n",
        "\n",
        "## To-Do\n",
        "\n",
        "1. Bonus: create a TF Dataset + don't load in RAM\n",
        "\n",
        "## Resources\n",
        "\n",
        "- https://arxiv.org/pdf/1412.6622.pdf\n",
        "- https://omoindrot.github.io/triplet-loss\n",
        "- https://www.coursera.org/lecture/convolutional-neural-networks/triplet-loss-HuUtN\n",
        "\n",
        "## Triplet networks\n",
        "\n",
        "### Definitions\n",
        "\n",
        "The aim of the triplet loss is to make sure that:\n",
        "- Two feature vectors with the same label have their embeddings close together in the embedding space.\n",
        "- Two feature vectors with different labels have their embeddings far away.\n",
        "\n",
        "To train the neural network we use triplets composed of:\n",
        "- an anchor\n",
        "- a positive with the same label as the anchor\n",
        "- a negative with a different label\n",
        "\n",
        "The loss function for a triplet (A, P, N) is:  \n",
        "$ L = max(d(A, P) - d(A, N) + margin, 0) $ \n",
        "\n",
        "### Triplet Mining\n",
        "\n",
        "The real trouble when implementing triplet loss or contrastive loss in TensorFlow is how to sample the triplets or pairs.\n",
        "\n",
        "1. Offline triplet mining : find them at the beginning of each epoch\n",
        "2. Online triplet mining : compute useful triplets on the fly\n",
        "\n",
        "### Implementations\n",
        "\n",
        "1. https://www.tensorflow.org/addons/tutorials/losses_triplet\n",
        "2. From scratch: https://stackoverflow.com/questions/38260113/implementing-contrastive-loss-and-triplet-loss-in-tensorflow/38270293#38270293\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9dO1rk15qSv",
        "colab_type": "text"
      },
      "source": [
        "## Link Google Drive to retrieve the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAask-525sdd",
        "colab_type": "code",
        "outputId": "7a312d8f-2862-4742-f72c-334c922d1f3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-77t7CWKXpj",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_KgdlkmAa9G",
        "colab_type": "code",
        "outputId": "9d2ece12-596a-4a0f-825e-72192c8bf922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "!pip install -q  --no-deps tensorflow-addons\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 2.8MB/s \n",
            "\u001b[?25h2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY4nsmb5KgtJ",
        "colab_type": "text"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-meFVpX7Dzu",
        "colab_type": "code",
        "outputId": "ea9f0ca8-68d2-4dce-e243-c6c9be120877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Load feature vectors\n",
        "vectors = np.fromfile('/content/drive/My Drive/ember-dataset/Xtrain.dat', dtype=np.float32)\n",
        "vectors = np.reshape(vectors, (-1, 2351)) # Create a 2d np array\n",
        "\n",
        "# Load labels\n",
        "labels = np.fromfile('/content/drive/My Drive/ember-dataset/Ytrain.dat', dtype=np.float32)\n",
        "\n",
        "# Remove unlabeled vectors\n",
        "unlabeled_index = np.argwhere(labels==-1).flatten()\n",
        "labels = np.delete(labels, unlabeled_index, 0)\n",
        "vectors = np.delete(vectors, unlabeled_index, 0)\n",
        "\n",
        "print (\"Vectors shape:\", vectors.shape)\n",
        "print (\"Labels shape:\", labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectors shape: (600000, 2351)\n",
            "Labels shape: (600000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yXZ763R7P2d",
        "colab_type": "text"
      },
      "source": [
        "## Split dataset (train + test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZkJ-s-p7NZO",
        "colab_type": "code",
        "outputId": "9da86396-81e5-4a59-d38e-85cfa8f45944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Split the dataset in two\n",
        "vectors_splitted = np.split(vectors, [500000])\n",
        "labels_splitted = np.split(labels, [500000])\n",
        "\n",
        "# Create a training dataset\n",
        "train_vectors = vectors_splitted[0]\n",
        "train_labels = labels_splitted[0]\n",
        "print (\"Train vectors shape:\", train_vectors.shape)\n",
        "print (\"Train labels shape:\", train_labels.shape)\n",
        "\n",
        "# Create a test dataset\n",
        "test_vectors = vectors_splitted[1]\n",
        "test_labels = labels_splitted[1]\n",
        "print (\"Test vectors shape:\", test_vectors.shape)\n",
        "print (\"Test labels shape:\", test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train vectors shape: (500000, 2351)\n",
            "Train labels shape: (500000,)\n",
            "Test vectors shape: (100000, 2351)\n",
            "Test labels shape: (100000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acX7TjraKl3c",
        "colab_type": "text"
      },
      "source": [
        "## Create neural network\n",
        "\n",
        "Here we use the triplet semi-hard loss function from tensorflow_addons: https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/contrib/losses/metric_learning/triplet_semihard_loss.\n",
        "\n",
        "This function relies on semi-hard online mining.\n",
        "As a reminder:\n",
        "- semi-hard triplets: triplets for which the positive is closer to the anchor than the negative, but which still have loss.\n",
        "- online mining: compute useful triplets on the fly.\n",
        "\n",
        "We adapt our network's model by:\n",
        "1. Removing our output layer (we want embeddings at the end).\n",
        "2. Removing the activation function of the last layer.\n",
        "3. L2 normalizing the embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSnv6uuNC0al",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Dense(256, activation='relu'),\n",
        "\n",
        "  # No activation on final dense layer (previously relu)\n",
        "  tf.keras.layers.Dense(256, activation=None),\n",
        "  \n",
        "  # No ouput layer: keras.layers.Dense(2, activation='softmax')\n",
        "\n",
        "  # L2 normalize embeddings\n",
        "  tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) \n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "              loss=tfa.losses.TripletSemiHardLoss())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4xmgS3FKseL",
        "colab_type": "text"
      },
      "source": [
        "## Train the neural network\n",
        "\n",
        "Unfortunately, the loss does not seem to decrease over the training period even after trying multiple model topologies. The minimum we got is $\\simeq$ 0.85. Not using \"batches\" from the dataset may explain this phenomenon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WvpYVZtGUMo",
        "colab_type": "code",
        "outputId": "02eb3617-1764-4783-8f61-c18c541cbacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# Train the network\n",
        "history = model.fit(train_vectors, train_labels, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 500000 samples\n",
            "Epoch 1/5\n",
            "500000/500000 [==============================] - 78s 155us/sample - loss: 0.9837\n",
            "Epoch 2/5\n",
            "500000/500000 [==============================] - 77s 153us/sample - loss: 0.9057\n",
            "Epoch 3/5\n",
            "500000/500000 [==============================] - 76s 153us/sample - loss: 0.8606\n",
            "Epoch 4/5\n",
            "500000/500000 [==============================] - 77s 155us/sample - loss: 0.8583\n",
            "Epoch 5/5\n",
            "500000/500000 [==============================] - 77s 155us/sample - loss: 0.8537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdaB9jJ-k66X",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KQCkEsVGZk4",
        "colab_type": "code",
        "outputId": "bd6064d7-5420-4f44-d5fd-450571a26f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Evaluate the network\n",
        "results = model.predict(test_vectors)\n",
        "print(results.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100000, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSWWKpkEKweW",
        "colab_type": "text"
      },
      "source": [
        "## Use results for k-means\n",
        "\n",
        "By saving `test_vectors` and `test_labels` as binary files we are able to use them to train our k-means algorithm.\n",
        "\n",
        "To start k-means using this new dataset some parameters have to be changed: \n",
        "`./kmeans 2 20 1.0 256 100000 ../ember/vectors.dat out.dat`. The training is relatively quick because of the small dimension (`256`) and the small number of vectors (`10000`). **Furthermore, the accuracy is of `0.8601`**. \n",
        "\n",
        "It is significantly greater than the one we get with the whole dataset. This can be explained by the fact that we train and test on the same dataset but still the result is worthwhile as doing so on the whole dataset would not give such accuracy. Doing more experiemnts would confirm this hypothesis.\n",
        "\n",
        "Therefore, triplet networks seems to be very efficient to compute new feature vectors (embeddings) which, with their small dimension, can be used in parallel of other classification techniques (machine learning classifiers, deep neural networks, k-means algorithm, ...)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWRB1EATRvY1",
        "colab_type": "code",
        "outputId": "0185110b-5338-4988-e189-09b8e7f53987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Export results for k-means\n",
        "results.flatten().tofile(\"vectors.dat\")\n",
        "test_labels.tofile(\"labels.dat\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25600000,)\n",
            "(100000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnpxURSnWyvf",
        "colab_type": "text"
      },
      "source": [
        "## Results projection visualization\n",
        "\n",
        "**In order to download files quickly, run the cell below if length of results is 10000 not 100000.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA9iKyQYKPj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Export results for visualization in projector\n",
        "np.savetxt(\"vectors.tsv\", results, delimiter='\\t')\n",
        "out_m = io.open('labels.tsv', 'w', encoding='utf-8')\n",
        "for label in test_labels:\n",
        "    out_m.write(str(label) + \"\\n\")\n",
        "out_m.close()\n",
        "\n",
        "# vectors.tsv : 256 vectors on 10000 lines\n",
        "# labels.tsv : one label (0.0 or 1.0) on 10000 lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1lRYBwtnwmC",
        "colab_type": "text"
      },
      "source": [
        "We can save results to use them on https://projector.tensorflow.org/. The screenshot below represents 10000 plotted using UMAP (Uniform Manifold Approximation and Projection) for dimension reduction.\n",
        "\n",
        "As a reminder, **0** represents benign programs and **1** is for malicious programs. We can notice that most of the lines belongs either to the first or to the second class of programs.\n",
        "\n",
        "![screenshot](https://i.imgur.com/TSGiXfk.png)"
      ]
    }
  ]
}