# -*- coding: utf-8 -*-
"""eslr-deep_neural_network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TwfyeQhqQqtxPH224KzFxeUSDagQE9Mp

# Depp Neural network (ESLR Recruitment Project 2019)

## To-Do

1. Don't load in RAM (https://stackoverflow.com/questions/56573622/loading-large-data-into-tensorflow-2-0-without-loading-it-on-the-ram)

## Results of tests

#### <u>test1</u>

3 layers: 

          512    activation = relu
          512    activation = relu
          2      activation = softmax

          optimizer = adam
          loss function = sparse_categorical_crossentropy
          metrics = accuracy

results:  

          for the training set: 
                loss = 7081.8095
                accuracy = 0.5130

          for the testing set:
                time: 18s
                loss = 15.6502
                accuracy = 0.4639

####<u>test2</u>

3 layers: 

          256    activation = relu
          256    activation = relu
          2      activation = softmax

          optimizer = adam
          loss function = sparse_categorical_crossentropy
          metrics = accuracy

results:  

          for the training set: 
                loss = 20.7225
                accuracy = 0.5094

          for the testing set:
                time: 4s
                loss = 4.1440
                accuracy = 0.5955

####<u>test3</u>

3 layers: 

          256    activation = relu
          256    activation = relu
          2      activation = relu

          optimizer = adam
          loss function = sparse_categorical_crossentropy
          metrics = accuracy

results:  

          for the training set: 
                loss = 0.6931
                accuracy = 0.5093

          for the testing set:
                time: 4s
                loss = 0.6931
                accuracy = 0.4600

####<u>test4</u>

3 layers (same as test2): 

          256    activation = relu
          256    activation = relu
          2      activation = softmax

          optimizer = adam
          loss function = sparse_categorical_crossentropy
          metrics = accuracy

results:  

          for the training set: 
                loss = 60.5456
                accuracy = 0.5524

          for the testing set:
                time: 4s
                loss = 3.2901
                accuracy = 0.6051

####<u>test5</u>

4 layers: 

          256    activation = relu
          256    activation = relu
          256    activation = relu
          2      activation = softmax

          optimizer = adam
          loss function = sparse_categorical_crossentropy
          metrics = accuracy

results:  

          for the training set: 
                loss = 0.6931
                accuracy = 0.5074

          for the testing set:
                time: 4s
                loss = 0.6942
                accuracy = 0.4601

####<u>test6</u>

3 layers: 

          256    activation = relu
          256    activation = relu
          2      activation = softmax

          optimizer = adam
          loss function = poisson
          metrics = accuracy

results:  

          for the training set: 
                loss = 4.4725
                accuracy = 0.0000

          for the testing set:
                time: 5s
                loss = 0.8515
                accuracy = 0.0000


####<u>test7</u>

3 layers: 

          256    activation = relu
          256    activation = relu
          2      activation = softmax

          optimizer = adam
          loss function = sparse_categorical_crossentropy
          metrics = sparse_categorical_accuracy

results:  

          for the training set: 
                loss = 2042.6137
                accuracy = 0.05117

          for the testing set:
                time: 4s
                loss = 18.5914
                accuracy = 0.4636


####<u>test8</u>

3 layers: 

          256    activation = relu
          256    activation = relu
          2      activation = softmax

          optimizer = adam
          loss function = mean_loss
          metrics = sparse_categorical_accuracy and accuracy

results:  

          for the training set: 
                loss = 1441.0542
                accuracy = 0.5116

          for the testing set:
                time: 5s
                loss = 234.2823
                accuracy = 0.4640

####<u>test9</u>

3 layers: 

          256    activation = relu
          256    activation = relu
          2      activation = softmax

          optimizer = adam
          loss function = 'mean_squared_error'
          metrics = accuracy

results:  

          for the training set: 
                loss = 0.5000
                accuracy = 0.0000

          for the testing set:
                time: 4s
                loss = 0.5000
                accuracy = 0.0000


####<u>test10</u>

3 layers (same as test2 and test4): 

          256    activation = relu
          256    activation = relu
          2      activation = softmax

          optimizer = adam
          loss function = 'sparse_categorical_crossentropy'
          metrics = accuracy

results:  

          for the training set: 
                loss = 0.5000
                accuracy = 0.0000

          for the testing set:
                time: 4s
                loss = 0.5000
                accuracy = 0.0000

## Interpretation of results

Having a topology of 3 layers with 2 hidden layers of 256 neurons each and one last with 3 seems to be the best choice in our case, since it is the one that allies both, performances and results.

In deed, the algorithm is significantly faster with this topology in comparison with the other 2 tested: one with the same number of hidden layer and 512 neurons in each, and another one with the same number of neurons by layers but a third hidden one.

Moreover, the results show that this topology gives a better accuracy for at same time the training set and the testing set. Those accuracy indices, although not exactly equal, are way more similar than the ones given by the other configurations.

The chosen loss/cost function is the sparse_categorical_crossentropy one because it was the only function tested that gave goods or even acceptable result with the best accuracy around 0.6. The other ones such as the poisson's one and the 'mean_squared_error' function gave accuracies close to 0.0. The chosen cost function has also in this library its own quivalent for the metric  parameter asked by the model.compile function in order to calculate the accuracy of the neural network.

## Link Google Drive to retrieve the dataset
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Import libaries"""

from __future__ import absolute_import, division, print_function, unicode_literals

import tensorflow as tf
from tensorflow import keras
import numpy as np

print(tf.__version__)

"""## Load dataset"""

# 0 = benign
# 1 = malicious
class_names = ['Benign', 'Malicious']

# Load feature vectors
vectors = np.fromfile('/content/drive/My Drive/ember-dataset/Xtrain.dat', dtype=np.float32)
vectors = np.reshape(vectors, (-1, 2351)) # Create a 2d np array

# Load labels
labels = np.fromfile('/content/drive/My Drive/ember-dataset/Ytrain.dat', dtype=np.float32)

# Remove unlabeled vectors
unlabeled_index = np.argwhere(labels==-1).flatten()
labels = np.delete(labels, unlabeled_index, 0)
vectors = np.delete(vectors, unlabeled_index, 0)

print ("Vectors shape:", vectors.shape)
print ("Labels shape:", labels.shape)

"""## Split dataset (train + test)"""

# Split the dataset in two
vectors_splitted = np.split(vectors, [500000])
labels_splitted = np.split(labels, [500000])

# Create a training dataset
train_vectors = vectors_splitted[0]
train_labels = labels_splitted[0]
print ("Train vectors shape:", train_vectors.shape)
print ("Train labels shape:", train_labels.shape)

# Create a test dataset
test_vectors = vectors_splitted[1]
test_labels = labels_splitted[1]
print ("Test vectors shape:", test_vectors.shape)
print ("Test labels shape:", test_labels.shape)

"""## Create neural network"""

model = keras.Sequential([
    keras.layers.Dense(256, activation='relu'),
    keras.layers.Dense(256, activation='relu'),
    keras.layers.Dense(2, activation='softmax')
])

# optimizer: The technique of optimization chosen
# loss: The name of cost function to use
#       sparse_categorical_crossentropy gave the best results throughout the tests
#       and got a metric equivalence to calculate the accuracy
# metrics: List of metrics to be evaluated by the model during training and testing
model.compile(optimizer='adam',   
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

"""## Train the neural network"""

model.fit(train_vectors, train_labels, epochs=10)

"""## Evaluate the accuracy of the neural network"""

test_loss, test_acc = model.evaluate(test_vectors,  test_labels, verbose=2)

print('Accuracy:', test_acc)