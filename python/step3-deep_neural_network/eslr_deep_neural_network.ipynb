{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eslr-deep_neural_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuCbkGB_eAvr",
        "colab_type": "text"
      },
      "source": [
        "# Depp Neural network (ESLR Recruitment Project 2019)\n",
        "\n",
        "## To-Do\n",
        "\n",
        "1. Don't load in RAM (https://stackoverflow.com/questions/56573622/loading-large-data-into-tensorflow-2-0-without-loading-it-on-the-ram)\n",
        "\n",
        "## Results of tests\n",
        "\n",
        "#### <u>test1</u>\n",
        "\n",
        "3 layers: \n",
        "\n",
        "          512    activation = relu\n",
        "          512    activation = relu\n",
        "          2      activation = softmax\n",
        "\n",
        "          optimizer = adam\n",
        "          loss function = sparse_categorical_crossentropy\n",
        "          metrics = accuracy\n",
        "\n",
        "results:  \n",
        "\n",
        "          for the training set: \n",
        "                loss = 7081.8095\n",
        "                accuracy = 0.5130\n",
        "\n",
        "          for the testing set:\n",
        "                time: 18s\n",
        "                loss = 15.6502\n",
        "                accuracy = 0.4639\n",
        "\n",
        "####<u>test2</u>\n",
        "\n",
        "3 layers: \n",
        "\n",
        "          256    activation = relu\n",
        "          256    activation = relu\n",
        "          2      activation = softmax\n",
        "\n",
        "          optimizer = adam\n",
        "          loss function = sparse_categorical_crossentropy\n",
        "          metrics = accuracy\n",
        "\n",
        "results:  \n",
        "\n",
        "          for the training set: \n",
        "                loss = 20.7225\n",
        "                accuracy = 0.5094\n",
        "\n",
        "          for the testing set:\n",
        "                time: 4s\n",
        "                loss = 4.1440\n",
        "                accuracy = 0.5955\n",
        "\n",
        "####<u>test3</u>\n",
        "\n",
        "3 layers: \n",
        "\n",
        "          256    activation = relu\n",
        "          256    activation = relu\n",
        "          2      activation = relu\n",
        "\n",
        "          optimizer = adam\n",
        "          loss function = sparse_categorical_crossentropy\n",
        "          metrics = accuracy\n",
        "\n",
        "results:  \n",
        "\n",
        "          for the training set: \n",
        "                loss = 0.6931\n",
        "                accuracy = 0.5093\n",
        "\n",
        "          for the testing set:\n",
        "                time: 4s\n",
        "                loss = 0.6931\n",
        "                accuracy = 0.4600\n",
        "\n",
        "####<u>test4</u>\n",
        "\n",
        "3 layers (same as test2): \n",
        "\n",
        "          256    activation = relu\n",
        "          256    activation = relu\n",
        "          2      activation = softmax\n",
        "\n",
        "          optimizer = adam\n",
        "          loss function = sparse_categorical_crossentropy\n",
        "          metrics = accuracy\n",
        "\n",
        "results:  \n",
        "\n",
        "          for the training set: \n",
        "                loss = 60.5456\n",
        "                accuracy = 0.5524\n",
        "\n",
        "          for the testing set:\n",
        "                time: 4s\n",
        "                loss = 3.2901\n",
        "                accuracy = 0.6051\n",
        "\n",
        "####<u>test5</u>\n",
        "\n",
        "4 layers: \n",
        "\n",
        "          256    activation = relu\n",
        "          256    activation = relu\n",
        "          256    activation = relu\n",
        "          2      activation = softmax\n",
        "\n",
        "          optimizer = adam\n",
        "          loss function = sparse_categorical_crossentropy\n",
        "          metrics = accuracy\n",
        "\n",
        "results:  \n",
        "\n",
        "          for the training set: \n",
        "                loss = 0.6931\n",
        "                accuracy = 0.5074\n",
        "\n",
        "          for the testing set:\n",
        "                time: 4s\n",
        "                loss = 0.6942\n",
        "                accuracy = 0.4601\n",
        "\n",
        "####<u>test6</u>\n",
        "\n",
        "3 layers: \n",
        "\n",
        "          256    activation = relu\n",
        "          256    activation = relu\n",
        "          2      activation = softmax\n",
        "\n",
        "          optimizer = adam\n",
        "          loss function = poisson\n",
        "          metrics = accuracy\n",
        "\n",
        "results:  \n",
        "\n",
        "          for the training set: \n",
        "                loss = 4.4725\n",
        "                accuracy = 0.0000\n",
        "\n",
        "          for the testing set:\n",
        "                time: 5s\n",
        "                loss = 0.8515\n",
        "                accuracy = 0.0000\n",
        "\n",
        "\n",
        "####<u>test7</u>\n",
        "\n",
        "3 layers: \n",
        "\n",
        "          256    activation = relu\n",
        "          256    activation = relu\n",
        "          2      activation = softmax\n",
        "\n",
        "          optimizer = adam\n",
        "          loss function = sparse_categorical_crossentropy\n",
        "          metrics = sparse_categorical_accuracy\n",
        "\n",
        "results:  \n",
        "\n",
        "          for the training set: \n",
        "                loss = 2042.6137\n",
        "                accuracy = 0.05117\n",
        "\n",
        "          for the testing set:\n",
        "                time: 4s\n",
        "                loss = 18.5914\n",
        "                accuracy = 0.4636\n",
        "\n",
        "\n",
        "####<u>test8</u>\n",
        "\n",
        "3 layers: \n",
        "\n",
        "          256    activation = relu\n",
        "          256    activation = relu\n",
        "          2      activation = softmax\n",
        "\n",
        "          optimizer = adam\n",
        "          loss function = mean_loss\n",
        "          metrics = sparse_categorical_accuracy and accuracy\n",
        "\n",
        "results:  \n",
        "\n",
        "          for the training set: \n",
        "                loss = 1441.0542\n",
        "                accuracy = 0.5116\n",
        "\n",
        "          for the testing set:\n",
        "                time: 5s\n",
        "                loss = 234.2823\n",
        "                accuracy = 0.4640\n",
        "\n",
        "####<u>test9</u>\n",
        "\n",
        "3 layers: \n",
        "\n",
        "          256    activation = relu\n",
        "          256    activation = relu\n",
        "          2      activation = softmax\n",
        "\n",
        "          optimizer = adam\n",
        "          loss function = 'mean_squared_error'\n",
        "          metrics = accuracy\n",
        "\n",
        "results:  \n",
        "\n",
        "          for the training set: \n",
        "                loss = 0.5000\n",
        "                accuracy = 0.0000\n",
        "\n",
        "          for the testing set:\n",
        "                time: 4s\n",
        "                loss = 0.5000\n",
        "                accuracy = 0.0000\n",
        "\n",
        "\n",
        "####<u>test10</u>\n",
        "\n",
        "3 layers (same as test2 and test4): \n",
        "\n",
        "          256    activation = relu\n",
        "          256    activation = relu\n",
        "          2      activation = softmax\n",
        "\n",
        "          optimizer = adam\n",
        "          loss function = 'sparse_categorical_crossentropy'\n",
        "          metrics = accuracy\n",
        "\n",
        "results:  \n",
        "\n",
        "          for the training set: \n",
        "                loss = 0.5000\n",
        "                accuracy = 0.0000\n",
        "\n",
        "          for the testing set:\n",
        "                time: 4s\n",
        "                loss = 0.5000\n",
        "                accuracy = 0.0000\n",
        "\n",
        "## Interpretation of results\n",
        "\n",
        "Having a topology of 3 layers with 2 hidden layers of 256 neurons each and one last with 3 seems to be the best choice in our case, since it is the one that allies both, performances and results.\n",
        "\n",
        "In deed, the algorithm is significantly faster with this topology in comparison with the other 2 tested: one with the same number of hidden layer and 512 neurons in each, and another one with the same number of neurons by layers but a third hidden one.\n",
        "\n",
        "Moreover, the results show that this topology gives a better accuracy for at same time the training set and the testing set. Those accuracy indices, although not exactly equal, are way more similar than the ones given by the other configurations.\n",
        "\n",
        "The chosen loss/cost function is the sparse_categorical_crossentropy one because it was the only function tested that gave goods or even acceptable result with the best accuracy around 0.6. The other ones such as the poisson's one and the 'mean_squared_error' function gave accuracies close to 0.0. The chosen cost function has also in this library its own quivalent for the metric  parameter asked by the model.compile function in order to calculate the accuracy of the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0dbVTM8gpBM",
        "colab_type": "text"
      },
      "source": [
        "## Link Google Drive to retrieve the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ3y563laTlZ",
        "colab_type": "code",
        "outputId": "182eac1b-efbd-4a27-870e-9b8a2fff95da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be20_Rh2goM1",
        "colab_type": "text"
      },
      "source": [
        "## Import libaries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgUEnN-BZqo9",
        "colab_type": "code",
        "outputId": "6fcfb5aa-ddfb-4d13-92f3-6d07c4ae4684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcE2Ra3wfMin",
        "colab_type": "text"
      },
      "source": [
        "## Load dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDqhUcRmiw9V",
        "colab_type": "code",
        "outputId": "7fc5e44a-ae49-4f53-e7ab-2590f760d5d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# 0 = benign\n",
        "# 1 = malicious\n",
        "class_names = ['Benign', 'Malicious']\n",
        "\n",
        "# Load feature vectors\n",
        "vectors = np.fromfile('/content/drive/My Drive/ember-dataset/Xtrain.dat', dtype=np.float32)\n",
        "vectors = np.reshape(vectors, (-1, 2351)) # Create a 2d np array\n",
        "\n",
        "# Load labels\n",
        "labels = np.fromfile('/content/drive/My Drive/ember-dataset/Ytrain.dat', dtype=np.float32)\n",
        "\n",
        "# Remove unlabeled vectors\n",
        "unlabeled_index = np.argwhere(labels==-1).flatten()\n",
        "labels = np.delete(labels, unlabeled_index, 0)\n",
        "vectors = np.delete(vectors, unlabeled_index, 0)\n",
        "\n",
        "print (\"Vectors shape:\", vectors.shape)\n",
        "print (\"Labels shape:\", labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectors shape: (600000, 2351)\n",
            "Labels shape: (600000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtOZKMICjMLp",
        "colab_type": "text"
      },
      "source": [
        "## Split dataset (train + test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ6vVo0yjKcM",
        "colab_type": "code",
        "outputId": "36f1b557-10b3-473b-d0d3-535d8d1739c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Split the dataset in two\n",
        "vectors_splitted = np.split(vectors, [500000])\n",
        "labels_splitted = np.split(labels, [500000])\n",
        "\n",
        "# Create a training dataset\n",
        "train_vectors = vectors_splitted[0]\n",
        "train_labels = labels_splitted[0]\n",
        "print (\"Train vectors shape:\", train_vectors.shape)\n",
        "print (\"Train labels shape:\", train_labels.shape)\n",
        "\n",
        "# Create a test dataset\n",
        "test_vectors = vectors_splitted[1]\n",
        "test_labels = labels_splitted[1]\n",
        "print (\"Test vectors shape:\", test_vectors.shape)\n",
        "print (\"Test labels shape:\", test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train vectors shape: (500000, 2351)\n",
            "Train labels shape: (500000,)\n",
            "Test vectors shape: (100000, 2351)\n",
            "Test labels shape: (100000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmn4Ostef1w3",
        "colab_type": "text"
      },
      "source": [
        "## Create neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGDzVW18cN_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(256, activation='relu'),\n",
        "    keras.layers.Dense(256, activation='relu'),\n",
        "    keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# optimizer: The technique of optimization chosen\n",
        "# loss: The name of cost function to use\n",
        "#       sparse_categorical_crossentropy gave the best results throughout the tests\n",
        "#       and got a metric equivalence to calculate the accuracy\n",
        "# metrics: List of metrics to be evaluated by the model during training and testing\n",
        "model.compile(optimizer='adam',   \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q48uZ9Dxf_62",
        "colab_type": "text"
      },
      "source": [
        "## Train the neural network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkLGO5Zp2ep1",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_j2FguogFbW",
        "colab_type": "code",
        "outputId": "7b8b07d3-412e-4a22-80b1-f8abeba7ddb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "source": [
        "model.fit(train_vectors, train_labels, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train on 500000 samples\n",
            "Epoch 1/10\n",
            "500000/500000 [==============================] - 184s 369us/sample - loss: 146330.6771 - acc: 0.5354\n",
            "Epoch 2/10\n",
            "500000/500000 [==============================] - 111s 222us/sample - loss: 7008.5687 - acc: 0.5224\n",
            "Epoch 3/10\n",
            "500000/500000 [==============================] - 65s 130us/sample - loss: 4131.2619 - acc: 0.5245\n",
            "Epoch 4/10\n",
            "500000/500000 [==============================] - 61s 123us/sample - loss: 2075.4360 - acc: 0.5257\n",
            "Epoch 5/10\n",
            "500000/500000 [==============================] - 95s 190us/sample - loss: 1749.8087 - acc: 0.5233\n",
            "Epoch 6/10\n",
            "500000/500000 [==============================] - 131s 263us/sample - loss: 3428.7871 - acc: 0.5190\n",
            "Epoch 7/10\n",
            "500000/500000 [==============================] - 62s 124us/sample - loss: 1939.1813 - acc: 0.5159\n",
            "Epoch 8/10\n",
            "500000/500000 [==============================] - 61s 123us/sample - loss: 3534.8163 - acc: 0.5148\n",
            "Epoch 9/10\n",
            "500000/500000 [==============================] - 120s 240us/sample - loss: 1499.0214 - acc: 0.5137\n",
            "Epoch 10/10\n",
            "500000/500000 [==============================] - 60s 121us/sample - loss: 193.1701 - acc: 0.5142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0364c0b978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIP9ck7rgB4u",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the accuracy of the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IBc3yjAgJg4",
        "colab_type": "code",
        "outputId": "190bb65c-c920-4cd7-fa5b-b7009020e63c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_vectors,  test_labels, verbose=2)\n",
        "\n",
        "print('Accuracy:', test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100000/100000 - 4s - loss: 10.2904 - acc: 0.6068\n",
            "Accuracy: 0.60684\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}